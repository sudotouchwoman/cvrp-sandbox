{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import lns\n",
    "from lns import cvrp\n",
    "from lns import get_logger\n",
    "\n",
    "\n",
    "logger = get_logger(\"main\", level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook assumes that benchmarking\n",
    "# was already run using benchmark.py script\n",
    "# (actually, it expects 2 separate runs of that script, with baseline and tuned parameters)\n",
    "experiments = pd.read_csv(\"../data/benchmark.csv\")\n",
    "baseline_experiments = pd.read_csv(\"../data/baseline-benchmark.csv\")\n",
    "experiments[\"mape-best-percent\"] = experiments[\"mape-best\"].map(lambda x: x * 100)\n",
    "baseline_experiments[\"mape-best-percent\"] = baseline_experiments[\"mape-best\"].map(\n",
    "    lambda x: x * 100\n",
    ")\n",
    "experiments[\"mape-initial-percent\"] = experiments[\"mape-initial\"].map(lambda x: x * 100)\n",
    "baseline_experiments[\"mape-initial-percent\"] = baseline_experiments[\"mape-initial\"].map(\n",
    "    lambda x: x * 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.sort_values(by=\"mape-best\", ascending=False)[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments[\"exp\"] = \"tuned\"\n",
    "baseline_experiments[\"exp\"] = \"baseline\"\n",
    "df = pd.concat((experiments, baseline_experiments))\n",
    "df = df[~df[\"name\"].isin((\"E-n31-k7\", \"E-n13-k4\"))]\n",
    "df.groupby(by=[\"exp\", \"subset\"], as_index=False).aggregate(\n",
    "    {\"mape-best-percent\": \"mean\"}\n",
    ")\n",
    "df[\"mape-best-percent\"] = df[\"mape-best-percent\"].map(lambda x: np.round(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_set_mape(df: pd.DataFrame, title: str = \"\"):\n",
    "    df = df[df[\"mape-best\"] < 0.5]\n",
    "    df = df.groupby(by=[\"exp\", \"subset\"], as_index=False).aggregate(\n",
    "        {\"mape-best-percent\": \"mean\"}\n",
    "    )\n",
    "    df[\"mape-best-percent-round\"] = df[\"mape-best-percent\"].map(lambda x: f\"{x:.3f}%\")\n",
    "    fig = px.bar(\n",
    "        df,\n",
    "        x=\"subset\",\n",
    "        color=\"exp\",\n",
    "        y=\"mape-best-percent\",\n",
    "        barmode=\"group\",\n",
    "        text=\"mape-best-percent-round\",\n",
    "        title=title,\n",
    "    )\n",
    "\n",
    "    # fig.update_traces(textposition=\"outside\", textfont_size=12)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_set_mape(df, \"Mean MAPE by subset, 2 outliers from set E removed (%)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mape_scores(df: pd.DataFrame, title_prefix: str = \"\"):\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"dimension\",\n",
    "        y=[\"mape-best-percent\"],\n",
    "        color=\"subset\",\n",
    "        hover_data=\"name\",\n",
    "        title=title_prefix + \" best MAPE (%), lower is better\",\n",
    "    )\n",
    "\n",
    "    fig.add_hline(y=0.0, line_width=0.7, line_dash=None, line_color=\"black\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_iterations(df: pd.DataFrame, title_suffix: str = \"\"):\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"dimension\",\n",
    "        y=\"time\",\n",
    "        color=\"subset\",\n",
    "        hover_data=\"name\",\n",
    "        title=\"Execution time (s) / problem size\" + title_suffix,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_initial_solution(df: pd.DataFrame):\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"dimension\",\n",
    "        y=\"mape-initial-percent\",\n",
    "        color=\"subset\",\n",
    "        hover_data=\"name\",\n",
    "        title=\"MAPE (%) of initial solution / problem size\",\n",
    "    )\n",
    "\n",
    "    fig.add_hline(y=0.0, line_width=0.7, line_dash=None, line_color=\"black\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mape_scores(\n",
    "    baseline_experiments[~baseline_experiments[\"name\"].isin((\"E-n31-k7\", \"E-n13-k4\"))],\n",
    "    \"ALNS (baseline, random remove + greedy repair + random accept)\",\n",
    ").show()\n",
    "plot_mape_scores(\n",
    "    experiments[~experiments[\"name\"].isin((\"E-n31-k7\", \"E-n13-k4\"))],\n",
    "    \"ALNS (random remove / SISR + greedy repair + SA accept)\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iterations(baseline_experiments, \" (baseline)\").show()\n",
    "plot_iterations(experiments, \" (after hyperparameter tuning)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_initial_solution(experiments).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import pairwise\n",
    "\n",
    "\n",
    "def plot_solution_routes(p: cvrp.Problem, sol: cvrp.Solution, title: str):\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"x\": p.customers[:, 0],\n",
    "            \"y\": p.customers[:, 1],\n",
    "            \"customer\": range(len(p.customers)),\n",
    "            \"marker\": \"customer\",\n",
    "            \"demand\": p.demands,\n",
    "            \"marker_size\": 0.2,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df.loc[0, [\"marker\", \"marker_size\"]] = \"depot\", 3\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        symbol=\"marker\",\n",
    "        symbol_sequence=[\"star\", \"circle-open\"],\n",
    "        size=\"marker_size\",\n",
    "        hover_data=\"customer\",\n",
    "        title=title,\n",
    "        height=800,\n",
    "        width=1200,\n",
    "    )\n",
    "\n",
    "    for i, route in enumerate(sol.routes, start=1):\n",
    "        edge_x, edge_y = [], []\n",
    "\n",
    "        for a, b in pairwise([0] + route + [0]):\n",
    "            edge_x.append(p.customers[a, 0])\n",
    "            edge_x.append(p.customers[b, 0])\n",
    "            edge_x.append(None)\n",
    "\n",
    "            edge_y.append(p.customers[a, 1])\n",
    "            edge_y.append(p.customers[b, 1])\n",
    "            edge_y.append(None)\n",
    "\n",
    "        fig.add_scatter(\n",
    "            x=edge_x,\n",
    "            y=edge_y,\n",
    "            name=f\"Route {i}\",\n",
    "            showlegend=False,\n",
    "        )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark import benchmark_model, read_problem\n",
    "\n",
    "\n",
    "def better_alns_factory(\n",
    "    seed: int = 10, max_iterations: int = 10_000, max_runtime: float = 60\n",
    "):\n",
    "    # ensure reentrance\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    def solve(p: cvrp.Problem, initial: cvrp.Solution):\n",
    "        accept_criterion = lns.accept.SimulatedAnnealing.fit(\n",
    "            initial.cost,\n",
    "            worse=0.5,\n",
    "            accept_proba=0.1,\n",
    "            num_iters=max_iterations,\n",
    "            method=\"exponential\",\n",
    "        )\n",
    "\n",
    "        cfg = lns.operators.BasicDestroyConfig(\n",
    "            problem=p,\n",
    "            bounds=[min(5, 0.1 * p.dim), min(50, 0.5 * p.dim)],\n",
    "            rng=rng,\n",
    "        )\n",
    "\n",
    "        destroy_operators = [\n",
    "            lns.operators.RandomRemove(cfg),\n",
    "            lns.operators.SubstringRemoval(\n",
    "                max_substring_removals=2,\n",
    "                max_string_size=12,\n",
    "                cfg=cfg,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        repair_operators = [\n",
    "            lns.operators.GreedyRepair(\n",
    "                lns.operators.BasicRepairConfig(\n",
    "                    problem=p,\n",
    "                    rng=rng,\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        solver = lns.alns.ALNS(\n",
    "            accept=accept_criterion,\n",
    "            destroy_operators=destroy_operators,\n",
    "            repair_operators=repair_operators,\n",
    "        )\n",
    "\n",
    "        return solver.iterate(\n",
    "            initial,\n",
    "            max_iter=max_iterations,\n",
    "            max_runtime=max_runtime,\n",
    "            verbose=True,\n",
    "            handle_interrupts=False,\n",
    "        )\n",
    "\n",
    "    return solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_to_inspect = experiments.sort_values(by=\"mape-best\", ascending=False)[\n",
    "    [\"name\", \"mape-best\"]\n",
    "][:5]\n",
    "problems_to_inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iterations(solution: lns.alns.TracedSolution, optimal_cost: float, title: str):\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"best\": solution.best_costs,\n",
    "            \"running\": solution.iteration_costs,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    mape = (solution.best_solution.cost - optimal_cost) / optimal_cost\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        y=\"running\",\n",
    "        height=800,\n",
    "        width=1200,\n",
    "        title=f\"{title}: Solution progress: best MAPE: {mape * 100:.3f}%\",\n",
    "    )\n",
    "\n",
    "    fig.add_scatter(\n",
    "        y=df.best,\n",
    "        mode=\"lines\",\n",
    "        name=\"best\",\n",
    "    )\n",
    "\n",
    "    fig.add_hline(y=optimal_cost)\n",
    "    fig.update_traces(marker_size=2.5)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarking_reports = [\n",
    "    benchmark_model(\n",
    "        name,\n",
    "        p,\n",
    "        opt,\n",
    "        model=better_alns_factory(max_iterations=100_000, max_runtime=300),\n",
    "        builder=lns.construct.random_builder,\n",
    "    )\n",
    "    for name, p, opt in map(\n",
    "        lambda x: read_problem(x, root=pathlib.Path(\"../data/\")),\n",
    "        problems_to_inspect[\"name\"][2:],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, case, *_ = benchmarking_reports\n",
    "plot_solution_routes(\n",
    "    case.problem,\n",
    "    case.initial_solution,\n",
    "    title=f\"{case.name} initial solution (greedy NN with FPS seeding), MAPE {case.mape_init * 100:.3f}%\",\n",
    ").show()\n",
    "plot_solution_routes(\n",
    "    case.problem,\n",
    "    case.alns_solution.best_solution,\n",
    "    title=f\"{case.name} best ALNS solution: {case.alns_solution.best_solution.cost:.3f}\"\n",
    "    f\" ({case.alns_solution.iterations} iterations in {case.alns_solution.elapsed_time:3.2f}s, MAPE {case.mape * 100:.3f}%)\",\n",
    ").show()\n",
    "plot_solution_routes(\n",
    "    case.problem,\n",
    "    case.opt_solution,\n",
    "    title=f\"{case.problem.name} (optimal solution - {case.opt_solution.cost})\",\n",
    ").show()\n",
    "plot_iterations(\n",
    "    case.alns_solution,\n",
    "    case.opt_solution.cost,\n",
    "    title=case.name,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like this instance did not had enough time to be solved\n",
    "name = \"M-n121-k7\"\n",
    "_, p, opt = read_problem(name, root=pathlib.Path(\"../data/\"))\n",
    "report = benchmark_model(\n",
    "    name,\n",
    "    p,\n",
    "    opt,\n",
    "    model=better_alns_factory(max_iterations=100_000, max_runtime=300),\n",
    "    builder=lns.construct.random_builder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_solution_routes(\n",
    "    report.problem,\n",
    "    report.opt_solution,\n",
    "    title=f\"{report.problem.name} optimal solution\",\n",
    ").show()\n",
    "plot_solution_routes(\n",
    "    report.problem,\n",
    "    report.initial_solution,\n",
    "    title=f\"{report.problem.name} initial solution (random), MAPE {report.mape_init * 100:.3f}%\",\n",
    ").show()\n",
    "plot_solution_routes(\n",
    "    report.problem,\n",
    "    report.alns_solution.best_solution,\n",
    "    title=f\"{report.name} best ALNS solution: {report.alns_solution.best_solution.cost:.3f}\"\n",
    "    f\" ({report.alns_solution.iterations} iterations in {report.alns_solution.elapsed_time:3.2f}s, MAPE {report.mape * 100:.3f}%)\",\n",
    ").show()\n",
    "plot_iterations(\n",
    "    report.alns_solution,\n",
    "    report.opt_solution.cost,\n",
    "    title=report.name,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
